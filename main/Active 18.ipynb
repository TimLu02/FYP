{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a02d1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nToDo: \\n    1. save metrics to tensor file during each run \\n    2. include test\\n    4. load model for test \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ToDo: \n",
    "    1. save metrics to tensor file during each run \n",
    "    2. include test\n",
    "    4. load model for test \n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8af3c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Library Calls\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.util import montage \n",
    "from skimage.transform import rotate\n",
    "from sklearn.model_selection import train_test_split as ttt\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from pylab import *\n",
    "import gc\n",
    "import torchmetrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3924e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variable definition\n",
    "IMAGE_PATH = 'data/MICCAI_BraTS_2019_Data_Training'\n",
    "CSV_FILE='data/MICCAI_BraTS_2019_Data_Training/new.csv'\n",
    "MODEL_PATH = 'log/crossentropy/'\n",
    "\n",
    "# number of slices, meaning how many slices we are taking from one image file  \n",
    "SLICE_NUM = 10\n",
    "# Original Image Size\n",
    "IMAGE_SIZE=240\n",
    "# Image size after cropping\n",
    "HEIGHT=128\n",
    "WIDTH=144\n",
    "\n",
    "# Batch size constant\n",
    "BATCH_SIZE= 1\n",
    "# input_channel corresponds to the number of different types of images used: \n",
    "# e.g: T1, T2, T1_flair, etc. \n",
    "# only using one type of images at the moment\n",
    "INPUT_CHANNEL=2\n",
    "EPOCH = 60\n",
    "LEARNING_RATE = 0.001\n",
    "SLICE_STARTS=torch.load('data/slice_starts.pt')\n",
    "WEIGHT = torch.load('data/weight.pt')\n",
    "device = torch.device('cuda')\n",
    "LOAD_CURRENT = False\n",
    "LOAD_BEST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2ca4464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Data loader for easy data access. \n",
    "\n",
    "Output shape for get_item: X [slices, input_channels, W, H ]\n",
    "                           y [slices, W, H, ]\n",
    "Notes: the output channel for y has dimension of 1 and has value in range of (0,3).\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Simple helper function for retriving images given the path\n",
    "def get_image(image_path,image_cat,image_id,image_type):\n",
    "    t1_data=nib.load(os.path.join(image_path,image_cat,image_id,'')+image_id+'_t1.nii.gz').get_fdata()\n",
    "    t1ce_data=nib.load(os.path.join(image_path,image_cat,image_id,'')+image_id+'_t1ce.nii.gz').get_fdata()\n",
    "    t2_data=nib.load(os.path.join(image_path,image_cat,image_id,'')+image_id+'_t2.nii.gz').get_fdata()\n",
    "    seg_data=nib.load(image_path+'/'+image_cat+'/'+image_id+'/'+image_id+'_seg.nii.gz').get_fdata()\n",
    "    flair_data=nib.load(os.path.join(image_path,image_cat,image_id,'')+image_id+'_flair.nii.gz').get_fdata()\n",
    "    result={'t1':t1_data,'t1ce':t1ce_data,'t2':t2_data,'seg':seg_data,'flair':flair_data}\n",
    "   \n",
    "    return result[image_type]; \n",
    "\n",
    "\n",
    "class BraTS19Dataset(Dataset):\n",
    "    def __init__(self, csv_f,i_path,i_type):\n",
    "        self.source=pd.read_csv(csv_f)\n",
    "        self.image_path=i_path\n",
    "        self.type=i_type\n",
    "        self.batch=BATCH_SIZE\n",
    "    def __len__(self):\n",
    "        return len(self.source)\n",
    "     \n",
    "    def __getitem__(self,idx):\n",
    "        image_cat=self.source.iloc[idx,0]\n",
    "        image_id=self.source.iloc[idx,4]\n",
    "        starts=SLICE_STARTS[idx].int()\n",
    "    \n",
    "        \n",
    "        \n",
    "        # get the image and its corresponding mask \n",
    "        img = np.zeros((INPUT_CHANNEL,IMAGE_SIZE,IMAGE_SIZE,155))\n",
    "        \n",
    "        for i in range(INPUT_CHANNEL):\n",
    "            img[i]=get_image(self.image_path,image_cat,image_id,self.type[i])\n",
    "        \n",
    "     \n",
    "        mask=get_image(self.image_path,image_cat,image_id,'seg')\n",
    "        \n",
    "        \n",
    "        # change label 4 to 3 for easy index handling\n",
    "        \n",
    "        mask[mask==4]=3\n",
    "        \n",
    "        # initialize results arrays\n",
    "        X=np.zeros((INPUT_CHANNEL,HEIGHT,WIDTH,SLICE_NUM))\n",
    "        y=np.zeros((HEIGHT,WIDTH,SLICE_NUM))\n",
    "        X = img[:,50:50+HEIGHT,55:55+WIDTH,starts-5:starts-5+SLICE_NUM]\n",
    "        y = mask[50:50+HEIGHT,55:55+WIDTH,starts-5:starts-5+SLICE_NUM]\n",
    "        X=torch.from_numpy(X)\n",
    "        y=torch.from_numpy(y)\n",
    "        \n",
    "       \n",
    "       \n",
    "        y = y.long()\n",
    "#        y = F.one_hot(y,num_classes=4)\n",
    "        return (X.permute(3,0,1,2),y.permute(2,0,1))\n",
    "    \n",
    "def GetBraTS19Dataset(csv,ipath,itype,batch_size=1,shuffle=False,pin_memory=True):\n",
    "    data = BraTS19Dataset(csv,ipath,itype)\n",
    "    leng = [int(0.6* len(data)),int(0.2* len(data)),len(data) - int(0.6 * len(data))-int(0.2 * len(data))]\n",
    "    train,eva,test=torch.utils.data.random_split(data,leng)\n",
    "    train=DataLoader(train,batch_size=batch_size,shuffle=shuffle,pin_memory=pin_memory)\n",
    "    eva=DataLoader(eva,batch_size=batch_size,shuffle=shuffle,pin_memory=pin_memory)\n",
    "    test=DataLoader(eva,batch_size=batch_size,shuffle=shuffle,pin_memory=pin_memory)\n",
    "    return train,eva,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7fb4bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet model\n",
    "# inspired by https://github.com/Hsankesara/DeepResearch/blob/826901dfe72037b9e433dccd85fe459f8411df20/UNet/Unet.py\n",
    "class Unet(nn.Module):\n",
    "\n",
    "    def encoder_block(self,in_channels,out_channels,k=3,conv2d_pad1=(1,1),conv2d_pad2=(1,1)):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=k,padding=conv2d_pad1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.GroupNorm(4,out_channels),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size=k,padding=conv2d_pad2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.GroupNorm(4,out_channels),\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    # actual output channel size = output_channel/2\n",
    "    def decoder_block(self,in_channels,out_channels,k=3,s=2,conv2d_pad1=(1,1),conv2d_pad2=(1,1),convtrans2d_pad=(0,0)):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=k,padding=conv2d_pad1),\n",
    "            nn.GroupNorm(4,out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size=k,padding=conv2d_pad2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.GroupNorm(4,out_channels),\n",
    "            nn.ConvTranspose2d(out_channels,int(out_channels/2),kernel_size=2,stride=s,padding=convtrans2d_pad),\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def bottle_neck(self,in_channels,out_channels,k=3):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=k,stride=1,padding=1),\n",
    "            nn.GroupNorm(4,out_channels),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size=k,stride=1,padding=1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.GroupNorm(4,out_channels),\n",
    "            nn.ConvTranspose2d(out_channels,in_channels,kernel_size=2,stride=2,padding=(0,0),output_padding=(0,0)),\n",
    "        )\n",
    "        return block\n",
    "    \n",
    "        \n",
    "    def final_block(self,in_channels,out_channels,k):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,int(in_channels/2),kernel_size=k,padding=(1,1)),\n",
    "            nn.GroupNorm(4,int(in_channels/2)),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(int(in_channels/2),out_channels,kernel_size=k,padding=(1,1)),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.GroupNorm(4,out_channels),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size=1,stride=1),\n",
    "            nn.Softmax(dim=1),\n",
    "            \n",
    "        )\n",
    "        return block\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(Unet, self).__init__()\n",
    "        self.en1=self.encoder_block(in_channel,32)\n",
    "        self.max1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.en2=self.encoder_block(32,64)\n",
    "        self.max2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.en3=self.encoder_block(64,128)\n",
    "        self.max3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.en4=self.encoder_block(128,256)\n",
    "        self.max4 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.bn=self.bottle_neck(256,512)\n",
    "        self.dc4=self.decoder_block(512,256)\n",
    "        self.dc3=self.decoder_block(256,128)\n",
    "        self.dc2=self.decoder_block(128,64)\n",
    "        self.final=self.final_block(64,out_channel,k=3)\n",
    "   \n",
    "    def forward(self,x):\n",
    "        encoder_block1=self.en1(x)\n",
    "        mp1 = self.max1(encoder_block1)\n",
    "        encoder_block2=self.en2(mp1)\n",
    "        mp2 = self.max2(encoder_block2)\n",
    "        encoder_block3=self.en3(mp2)\n",
    "        mp3 = self.max3(encoder_block3)\n",
    "        encoder_block4=self.en4(mp3)\n",
    "        mp4 = self.max4(encoder_block4)\n",
    "        bottleneck_block1 =self.bn(mp4)\n",
    "        \n",
    "        cat_block4= torch.cat((bottleneck_block1,encoder_block4),1)\n",
    "        decoder_block4=self.dc4(cat_block4)\n",
    "        \n",
    "        cat_block3= torch.cat((decoder_block4,encoder_block3),1)\n",
    "        decoder_block3=self.dc3(cat_block3)\n",
    "        \n",
    "        cat_block2= torch.cat((decoder_block3,encoder_block2),1)\n",
    "        decoder_block2=self.dc2(cat_block2)\n",
    "        \n",
    "        cat_block1= torch.cat((decoder_block2,encoder_block1),1)\n",
    "        \n",
    "        result=self.final(cat_block1)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c16e88bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiceScore(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiceScore, self).__init__()\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        dice = torchmetrics.Dice(average = 'micro').to(device)\n",
    "        macdice = torchmetrics.Dice(num_classes=4,average = 'micro').to(device)\n",
    "        assert y_pred.size() == y_true.size()\n",
    "        y_pred1 = F.one_hot(torch.argmax(y_pred,dim=3),num_classes=4)\n",
    "      \n",
    "        yp0 = y_pred1[:,:,:,0].contiguous().view(-1)\n",
    "        yt0 = y_true[:,:,:,0].contiguous().view(-1)\n",
    "        t0 = dice(yp0,yt0)\n",
    "        \n",
    "        yp1 = y_pred1[:,:,:,1].contiguous().view(-1)\n",
    "        yt1 = y_true[:,:,:,1].contiguous().view(-1)\n",
    "        t1 = dice(yp1,yt1)\n",
    "        \n",
    "        yp2 = y_pred1[:,:,:,2].contiguous().view(-1)\n",
    "        yt2 = y_true[:,:,:,2].contiguous().view(-1)\n",
    "        t2 = dice(yp2,yt2)\n",
    "        \n",
    "    \n",
    "        yp3 = y_pred1[:,:,:,3].contiguous().view(-1)\n",
    "        yt3 = y_true[:,:,:,3].contiguous().view(-1)\n",
    "        t3 = dice(yp3,yt3)\n",
    "\n",
    "\n",
    "        return macdice(y_pred1,y_true),t0,t1,t2,t3\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "434a4992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        img1=np.zeros((HEIGHT,WIDTH))\\n        img1=torch.tensor(img1)\\n        img1[:,:]=res.argmax(dim=1)[5,:,:]\\n        img1[img1==3]=4\\n        img2=np.zeros((HEIGHT,WIDTH))\\n        img2=torch.tensor(img2)\\n        img2[:,:]=y[5,:,:]\\n        img2[img2==3]=4\\n        figure()\\n        imshow(img1)\\n        show()\\n        figure()\\n        imshow(img2)\\n        show()\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train function\n",
    "def training(dataset, model, opt,  dev, batch_size, lr):\n",
    "    print('Training in session: ')\n",
    "    iou=torchmetrics.JaccardIndex('multiclass',num_classes=4,validate_args=False).to(device)\n",
    "    dic = DiceScore().to(device)\n",
    "    IoU=[]\n",
    "    md=[]\n",
    "    d0=[]\n",
    "    d1=[]\n",
    "    d2=[]\n",
    "    d3=[]\n",
    "    epoch_loss=0.0\n",
    "    dataset=tqdm(dataset)\n",
    "    for index, batch in enumerate(dataset):\n",
    "        \n",
    "        # reshape the data and load the data to gpu\n",
    "        x, y= batch \n",
    "        a,b,c,d,e=x.shape\n",
    "        x=x.reshape(a*b,c,d,e)\n",
    "        x=x.float()\n",
    "\n",
    "        a,b,c,d=y.shape\n",
    "        y=y.reshape(a*b,c,d)\n",
    "        \n",
    "        x,y=x.to(device),y.to(device)\n",
    "       \n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        loss_fn = nn..CrossEntropyLoss().to(device)\n",
    "       \n",
    "        # training model\n",
    "        res = model(x)\n",
    "        res=res.to(device)\n",
    "        loss = loss_fn(res.permute(0,2,3,1).to(device),F.one_hot(y,num_classes=4).to(device)).to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), gc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Loss \n",
    "        print(\"Batch_id: \", index,\"Training loss: \", loss.item())\n",
    "        epoch_loss+=loss.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #IoU\n",
    "        bac = iou(torch.argmax(res,dim = 1).to(device),y.to(device)).to(device)\n",
    "        print(\"Batch_id: \", index,\"Training IoU: \", bac)\n",
    "        IoU.append(bac)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Dice Score\n",
    "        \n",
    "        t,t0,t1,t2,t3=dic(res.permute(0,2,3,1).to(device),F.one_hot(y,num_classes=4).to(device))\n",
    "        \n",
    "        \n",
    "            \n",
    "        print(\"Batch_id: \", index,\"Training Mean Dice Score: \", t)\n",
    "        md.append(t)\n",
    "            \n",
    "        print(\"Batch_id: \", index,\"Training Non-tumor Dice Score: \", t0)\n",
    "        d0.append(t0)\n",
    "            \n",
    "        print(\"Batch_id: \", index,\"Training NCR/NET Dice Score: \", t1)\n",
    "        d1.append(t1)\n",
    "            \n",
    "        print(\"Batch_id: \", index,\"Training ED Dice Score: \", t2)\n",
    "        d2.append(t2)\n",
    "            \n",
    "        print(\"Batch_id: \", index,\"Training ET Dice Score: \", t3)\n",
    "        d3.append(t3)\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "    IoU=torch.tensor(IoU)\n",
    "    md = torch.tensor(md)\n",
    "    d0=torch.tensor(d0)\n",
    "    d1=torch.tensor(d1)\n",
    "    d2=torch.tensor(d2)\n",
    "    d3=torch.tensor(d3)\n",
    "    return torch.mean(IoU),epoch_loss,torch.mean(md),torch.mean(d0),torch.mean(d1),torch.mean(d2),torch.mean(d3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "        img1=np.zeros((HEIGHT,WIDTH))\n",
    "        img1=torch.tensor(img1)\n",
    "        img1[:,:]=res.argmax(dim=1)[5,:,:]\n",
    "        img1[img1==3]=4\n",
    "        img2=np.zeros((HEIGHT,WIDTH))\n",
    "        img2=torch.tensor(img2)\n",
    "        img2[:,:]=y[5,:,:]\n",
    "        img2[img2==3]=4\n",
    "        figure()\n",
    "        imshow(img1)\n",
    "        show()\n",
    "        figure()\n",
    "        imshow(img2)\n",
    "        show()\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"        \n",
    "        \n",
    "        \n",
    "        \n",
    "#        IoU.append(iou(res,y.float()).to(torch.device('cpu')))\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c332a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluation(dataset, model):\n",
    "    print('Evaluation in session: ')\n",
    "    iou=torchmetrics.JaccardIndex('multiclass',num_classes=4,validate_args=False).to(device)\n",
    "    dic = DiceScore().to(device)\n",
    "    IoU=[]\n",
    "    md=[]\n",
    "    d0=[]\n",
    "    d1=[]\n",
    "    d2=[]\n",
    "    d3=[]\n",
    "    epoch_loss=0.0\n",
    "    dataset=tqdm(dataset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for index, batch in enumerate(dataset):\n",
    "        \n",
    "            # reshape the data and load the data to gpu\n",
    "            x, y= batch \n",
    "            a,b,c,d,e=x.shape\n",
    "            x=x.reshape(a*b,c,d,e)\n",
    "            x=x.float()\n",
    "        \n",
    "            a,b,c,d=y.shape\n",
    "            y=y.reshape(a*b,c,d)\n",
    "        \n",
    "            x,y=x.to(device),y.to(device)\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "       \n",
    "            #eval\n",
    "            res = model(x)\n",
    "            res=res.to(device)\n",
    "            loss = loss_fn(res.permute(0,2,3,1).to(device),F.one_hot(y,num_classes=4).to(device)).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Loss\n",
    "            print(\"Batch_id: \", index,\"Eval loss: \", loss.item())\n",
    "            epoch_loss+=loss.item()\n",
    "            \n",
    "            \n",
    "            # IoU\n",
    "\n",
    "            bac = iou(torch.argmax(res,dim = 1).to(device),y.to(device)).to(device)\n",
    "            print(\"Batch_id: \", index,\"Eval IoU: \", bac)\n",
    "            IoU.append(bac)\n",
    "            \n",
    "            \n",
    "            # Dice Score\n",
    "            t,t0,t1,t2,t3=dic(res.permute(0,2,3,1).to(device),F.one_hot(y,num_classes=4).to(device))\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Eval Mean Dice Score: \", t)\n",
    "            md.append(t)\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Eval Non-tumor Dice Score: \", t0)\n",
    "            d0.append(t0)\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Eval NCR/NET Dice Score: \", t1)\n",
    "            d1.append(t1)\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Eval ED Dice Score: \", t2)\n",
    "            d2.append(t2)\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Eval ET Dice Score: \", t3)\n",
    "            d3.append(t3)\n",
    "            \n",
    "            img1=np.zeros((HEIGHT,WIDTH))\n",
    "            img1=torch.tensor(img1)\n",
    "            img1[:,:]=res.argmax(dim=1)[5,:,:]\n",
    "            img1[img1==3]=4\n",
    "            img2=np.zeros((HEIGHT,WIDTH))\n",
    "            img2=torch.tensor(img2)\n",
    "            img2[:,:]=y[5,:,:]\n",
    "            img2[img2==3]=4\n",
    "            figure()\n",
    "            imshow(img1)\n",
    "            show()\n",
    "            figure()\n",
    "            imshow(img2)\n",
    "            show()\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "    IoU=torch.tensor(IoU)\n",
    "    md = torch.tensor(md)\n",
    "    d0=torch.tensor(d0)\n",
    "    d1=torch.tensor(d1)\n",
    "    d2=torch.tensor(d2)\n",
    "    d3=torch.tensor(d3)\n",
    "    return torch.mean(IoU),epoch_loss,torch.mean(md),torch.mean(d0),torch.mean(d1),torch.mean(d2),torch.mean(d3)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad509699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Best Model\n",
    "def test(dataset):\n",
    "    print('test in session: ') \n",
    "    checkpoint = torch.load(MODEL_PATH+'#18-BEST')\n",
    "    model = Unet(in_channel=INPUT_CHANNEL,out_channel=4).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    opt.load_state_dict(checkpoint['optim_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    Avg_dice = checkpoint['Eval Dice Scores']\n",
    "    non = checkpoint['Eval Non-tumor Dice Scores']\n",
    "    ncr = checkpoint['Eval NCR/NET Dice Scores']\n",
    "    ed = checkpoint['Eval ED Dice Scores']\n",
    "    et = checkpoint['Eval ET Dice Scores']\n",
    "    \n",
    "    print('Model loaded!','Epoch = ', epoch, 'Mean eval dice scores is: ',Avg_dice)\n",
    "    print('Dice Scorese Per class: ')\n",
    "    print('Non-Tumor: ', non)\n",
    "    print('NCR/NET: ', ncr)\n",
    "    print('ED: ', ed)\n",
    "    print('ET: ',et)\n",
    "    iou=torchmetrics.JaccardIndex('multiclass',num_classes=4,validate_args=False).to(device)\n",
    "    dic = DiceScore().to(device)\n",
    "    IoU=[]\n",
    "    md=[]\n",
    "    d0=[]\n",
    "    d1=[]\n",
    "    d2=[]\n",
    "    d3=[]\n",
    "    epoch_loss=0.0\n",
    "    dataset=tqdm(dataset)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for index, batch in enumerate(dataset):\n",
    "        \n",
    "            # reshape the data and load the data to gpu\n",
    "            x, y= batch \n",
    "            a,b,c,d,e=x.shape\n",
    "            x=x.reshape(a*b,c,d,e)\n",
    "            x=x.float()\n",
    "        \n",
    "            a,b,c,d=y.shape\n",
    "            y=y.reshape(a*b,c,d)\n",
    "        \n",
    "            x,y=x.to(device),y.to(device)\n",
    "       \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "       \n",
    "            #test\n",
    "            res = model(x)\n",
    "            res=res.to(device)\n",
    "            loss = loss_fn(res.permute(0,2,3,1).to(device),F.one_hot(y,num_classes=4).to(device)).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "            # Loss\n",
    "            print(\"Batch_id: \", index,\"Test loss: \", loss.item())\n",
    "            epoch_loss+=loss.item()\n",
    "            \n",
    "            \n",
    "            # IoU\n",
    "\n",
    "            bac = iou(torch.argmax(res,dim = 1).to(device),y.to(device)).to(device)\n",
    "            print(\"Batch_id: \", index,\"Test IoU: \", bac)\n",
    "            IoU.append(bac)\n",
    "            \n",
    "            \n",
    "            # Dice Score\n",
    "            t,t0,t1,t2,t3=dic(res.permute(0,2,3,1).to(device),F.one_hot(y,num_classes=4).to(device))\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Test Mean Dice Score: \", t)\n",
    "            md.append(t)\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Test Non-tumor Dice Score: \", t0)\n",
    "            d0.append(t0)\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Test NCR/NET Dice Score: \", t1)\n",
    "            d1.append(t1)\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Test ED Dice Score: \", t2)\n",
    "            d2.append(t2)\n",
    "            \n",
    "            print(\"Batch_id: \", index,\"Test ET Dice Score: \", t3)\n",
    "            d3.append(t3)\n",
    "\n",
    "            img1=np.zeros((HEIGHT,WIDTH))\n",
    "            img1=torch.tensor(img1)\n",
    "            img1[:,:]=res.argmax(dim=1)[0,:,:]\n",
    "            img1[img1==3]=4\n",
    "            img2=np.zeros((HEIGHT,WIDTH))\n",
    "            img2=torch.tensor(img2)\n",
    "            img2[:,:]=y[0,:,:]\n",
    "            img2[img2==3]=4\n",
    "            figure()\n",
    "            imshow(img1)\n",
    "            show()\n",
    "            figure()\n",
    "            imshow(img2)\n",
    "            show()\n",
    "            \n",
    "    IoU=torch.tensor(IoU)\n",
    "    md = torch.tensor(md)\n",
    "    d0=torch.tensor(d0)\n",
    "    d1=torch.tensor(d1)\n",
    "    d2=torch.tensor(d2)\n",
    "    d3=torch.tensor(d3)\n",
    "    print('Avg IoU: ',torch.mean(IoU))\n",
    "    print('Avg Loss: ',epoch_loss/(BATCH_SIZE*SLICE_NUM))\n",
    "    print('Avg Dice Scores: ', torch.mean(md))\n",
    "    print('Non-Tumor Dice Scores: ',torch.mean(d0))\n",
    "    print('NCR/NET Dice Scores: ',torch.mean(d1))\n",
    "    print('ED Dice Scores: ',torch.mean(d2))\n",
    "    print('ET Dice Scores: ', torch.mean(d3))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3922143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "\n",
    "def main(BEST_EVAL):\n",
    "    if LOAD_CURRENT:\n",
    "        train_data = torch.load(MODEL_PATH+'trdata.pth')\n",
    "        eval_data = torch.load(MODEL_PATH+'edata.pth')\n",
    "        test_data = torch.load(MODEL_PATH+'tdata.pth')\n",
    "    else:\n",
    "        train_data,eval_data,test_data = GetBraTS19Dataset(CSV_FILE,IMAGE_PATH,['t1ce','flair'],batch_size=BATCH_SIZE)\n",
    "        torch.save(train_data, MODEL_PATH+'trdata.pth')\n",
    "        torch.save(eval_data, MODEL_PATH+'edata.pth')\n",
    "        torch.save(test_data,MODEL_PATH+'tdata.pth')\n",
    "    \n",
    "    print('Data Loaded!')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = Unet(in_channel=INPUT_CHANNEL,out_channel=4).to(device)\n",
    "    opt = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    sc = torch.optim.lr_scheduler.ReduceLROnPlateau(opt,patience=5,factor = 0.5,threshold=0.1,mode = 'max',verbose = True)\n",
    "    if LOAD_CURRENT:\n",
    "        checkpoint = torch.load(MODEL_PATH+'#18-current')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        opt.load_state_dict(checkpoint['optim_state_dict'])\n",
    "        sc.load_state_dict(checkpoint['sche'])\n",
    "        index =torch.load(MODEL_PATH+'index.pt')\n",
    "        loss_value=torch.load(MODEL_PATH+'tloss.pt')\n",
    "        accuracy=torch.load(MODEL_PATH+'tac.pt')\n",
    "        eval_loss=torch.load(MODEL_PATH+'eloss.pt')\n",
    "        eval_accuracy=torch.load(MODEL_PATH+'eacc.pt')\n",
    "        mean_dice=torch.load(MODEL_PATH+'md.pt')\n",
    "        d0=torch.load(MODEL_PATH+'d0.pt')\n",
    "        d1=torch.load(MODEL_PATH+'d1.pt')\n",
    "        d2=torch.load(MODEL_PATH+'d2.pt')\n",
    "        d3=torch.load(MODEL_PATH+'d3.pt')\n",
    "        e_mean_dice=torch.load(MODEL_PATH+'emd.pt')\n",
    "        ed0=torch.load(MODEL_PATH+'ed0.pt')\n",
    "        ed1=torch.load(MODEL_PATH+'ed1.pt')\n",
    "        ed2=torch.load(MODEL_PATH+'ed2.pt')\n",
    "        ed3=torch.load(MODEL_PATH+'ed3.pt')     \n",
    "        ep = torch.load(MODEL_PATH+'ep.pt')\n",
    "        \n",
    "    else:\n",
    "        index =[]\n",
    "        loss_value=[]\n",
    "        accuracy=[]\n",
    "        eval_loss=[]\n",
    "        eval_accuracy=[]\n",
    "        mean_dice=[]\n",
    "        d0=[]\n",
    "        d1=[]\n",
    "        d2=[]\n",
    "        d3=[]\n",
    "        e_mean_dice=[]\n",
    "        ed0=[]\n",
    "        ed1=[]\n",
    "        ed2=[]\n",
    "        ed3=[]\n",
    "        ep = 0 \n",
    "    \n",
    "    \n",
    "    for i in range (ep,EPOCH):\n",
    "        print(f'Epoch: {i}')\n",
    "        \n",
    "        acc, loss_val,md,di0,di1,di2,di3=training(train_data,model.train(),opt, device,BATCH_SIZE,LEARNING_RATE)\n",
    "        \n",
    "        eacc,eloss_val,emd,edi0,edi1,edi2,edi3=evaluation(eval_data,model.eval())\n",
    "        sc.step(eloss_val)\n",
    "        \n",
    "        loss_value.append(loss_val/len(train_data.sampler))\n",
    "        accuracy.append(acc)\n",
    "        mean_dice.append(md)\n",
    "        d0.append(di0)\n",
    "        d1.append(di1)\n",
    "        d2.append(di2)\n",
    "        d3.append(di3)\n",
    "        \n",
    "        eval_loss.append(eloss_val/len(eval_data.sampler))\n",
    "        eval_accuracy.append(eacc)\n",
    "        e_mean_dice.append(emd)\n",
    "        ed0.append(edi0)\n",
    "        ed1.append(edi1)\n",
    "        ed2.append(edi2)\n",
    "        ed3.append(edi3)\n",
    "        \n",
    "        index.append(i)\n",
    "        ep=ep+1\n",
    "        \n",
    "        torch.save(index,MODEL_PATH+'index.pt')\n",
    "        torch.save(loss_value,MODEL_PATH+'tloss.pt')\n",
    "        torch.save(accuracy,MODEL_PATH+'tac.pt')\n",
    "        torch.save(eval_loss,MODEL_PATH+'eloss.pt')\n",
    "        torch.save(eval_accuracy,MODEL_PATH+'eacc.pt')\n",
    "        torch.save(mean_dice,MODEL_PATH+'md.pt')\n",
    "        torch.save(d0,MODEL_PATH+'d0.pt')\n",
    "        torch.save(d1,MODEL_PATH+'d1.pt')\n",
    "        torch.save(d2,MODEL_PATH+'d2.pt')\n",
    "        torch.save(d3,MODEL_PATH+'d3.pt')\n",
    "        torch.save(e_mean_dice,MODEL_PATH+'emd.pt')\n",
    "        torch.save(ed0,MODEL_PATH+'ed0.pt')\n",
    "        torch.save(ed1,MODEL_PATH+'ed1.pt')\n",
    "        torch.save(ed2,MODEL_PATH+'ed2.pt')\n",
    "        torch.save(ed3,MODEL_PATH+'ed3.pt')     \n",
    "        torch.save(ep,MODEL_PATH+'ep.pt')\n",
    "        \n",
    "        \n",
    "        print('Epoch ',i,'Training loss: ',loss_val/len(train_data.sampler))\n",
    "        print('Epoch ',i,'Evaluation loss: ',eloss_val/len(eval_data.sampler))\n",
    "        \n",
    "        print('Epoch ',i,'Training accuracy : ',acc)\n",
    "        print('Epoch ',i,'Evaluation accuracy : ',eacc)\n",
    "        \n",
    "        print('Epoch ',i,'Training Mean Dice Score: ',md)\n",
    "        print('Epoch ',i,'Eval Mean Dice Score: ',emd)\n",
    "        \n",
    "        print('Epoch ',i,'Training Non-tumor Dice Score: ',di0)\n",
    "        print('Epoch ',i,'Eval Non-tumor Dice Score: ',edi0)\n",
    "       \n",
    "        print('Epoch ',i,'Training NCR/NET Dice Score: ',di1)\n",
    "        print('Epoch ',i,'Eval NCR/NET Dice Score: ',edi1)\n",
    "        \n",
    "        print('Epoch ',i,'Training ED Dice Score: ',di2)\n",
    "        print('Epoch ',i,'Eval ED Dice Score: ',edi2)\n",
    "        \n",
    "        print('Epoch ',i,'Training ET Dice Score: ',di3)\n",
    "        print('Epoch ',i,'Eval ET Dice Score: ',edi3)\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optim_state_dict': opt.state_dict(),\n",
    "            'epoch': i,\n",
    "            'sche':sc.state_dict(),\n",
    "            'Training loss_values': loss_val/len(train_data.sampler),\n",
    "            'Training accuracy':acc,\n",
    "            'Eval loss_values': eloss_val/len(eval_data.sampler),\n",
    "            'Eval IoU':eacc,\n",
    "            'Training Dice Scores':md,\n",
    "            'Eval Dice Scores': emd,\n",
    "            'Training Non-tumor Dice Scores':di0,\n",
    "            'Eval Non-tumor Dice Scores':edi0,   \n",
    "            'Training NCR/NET Dice Scores': di1,\n",
    "            'Eval NCR/NET Dice Scores': edi1,   \n",
    "            'Training ED Dice Scores': di2,\n",
    "            'Eval ED Dice Scores': edi2,    \n",
    "            'Training ET Dice Scores': di3,\n",
    "            'Eval ET Dice Scores': edi3,    \n",
    "            }, MODEL_PATH+'#18-current')\n",
    "        print('Epoch completed and model successfully saved')\n",
    "        if BEST_EVAL < eacc :\n",
    "            torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optim_state_dict': opt.state_dict(),\n",
    "            'epoch': i,\n",
    "            'sche':sc.state_dict(),\n",
    "            'Training loss_values': loss_val/len(train_data.sampler),\n",
    "            'Training accuracy':acc,\n",
    "            'Eval loss_values': eloss_val/len(eval_data.sampler),\n",
    "            'Eval IoU':eacc,\n",
    "            'Training Dice Scores':md,\n",
    "            'Eval Dice Scores': emd,\n",
    "            'Training Non-tumor Dice Scores':di0,\n",
    "            'Eval Non-tumor Dice Scores':edi0,   \n",
    "            'Training NCR/NET Dice Scores': di1,\n",
    "            'Eval NCR/NET Dice Scores': edi1,   \n",
    "            'Training ED Dice Scores': di2,\n",
    "            'Eval ED Dice Scores': edi2,    \n",
    "            'Training ET Dice Scores': di3,\n",
    "            'Eval ET Dice Scores': edi3,    \n",
    "            }, MODEL_PATH+'#18-BEST')\n",
    "            print(\"Best Model saved!\")\n",
    "            BEST_EVAL = eacc\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        if eacc > 0.75:\n",
    "            break\n",
    "        \n",
    "    plt.plot(index, loss_value, label =\"Training \")\n",
    "    plt.plot(index,eval_loss, label = \"Validation\")\n",
    "    plt.title(\"Training and Validation Loss Curve: (Sample_size={}, lr={})\".format(BATCH_SIZE*SLICE_NUM ,LEARNING_RATE))\n",
    "    plt.xlabel(\"Epoch:\")\n",
    "    plt.ylabel(\"Average Loss \")\n",
    "    plt.legend()\n",
    "    plt.savefig(MODEL_PATH+ 'Training and eval Loss plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot(index, accuracy,label =\"Training \")\n",
    "    plt.plot(index, eval_accuracy,label =\"Validation \")\n",
    "    plt.title(\"Training and Validation IoU Scores (batch_size={}, lr={})\".format(BATCH_SIZE*SLICE_NUM, LEARNING_RATE))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (Mean IoU Scores)\")\n",
    "    plt.legend()\n",
    "    plt.savefig(MODEL_PATH+ 'Training and eval Accuracy plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(index, mean_dice, label =\"Training \")\n",
    "    plt.plot(index,e_mean_dice, label = \"Validation\")\n",
    "    plt.title(\"Training and Validation Mean Dice Scores: (Sample_size={}, lr={})\".format(BATCH_SIZE*SLICE_NUM ,LEARNING_RATE))\n",
    "    plt.xlabel(\"Epoch:\")\n",
    "    plt.ylabel(\"Mean Dice Scores: \")\n",
    "    plt.legend()\n",
    "    plt.savefig(MODEL_PATH+ 'Training and eval avg dice plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(index, d0, label =\"Non-Tumor \")\n",
    "    plt.plot(index,d1, label = \"NCT/NET\")\n",
    "    plt.plot(index, d2, label =\"ED \")\n",
    "    plt.plot(index,d3, label = \"ET\")\n",
    "    plt.title(\"Training Dice Scores per class: (Sample_size={}, lr={})\".format(BATCH_SIZE*SLICE_NUM ,LEARNING_RATE))\n",
    "    plt.xlabel(\"Epoch:\")\n",
    "    plt.ylabel(\"Mean Dice Scores: \")\n",
    "    plt.legend()\n",
    "    plt.savefig(MODEL_PATH+ 'Training dice per class plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(index, ed0, label =\"Non-Tumor \")\n",
    "    plt.plot(index,ed1, label = \"NCT/NET\")\n",
    "    plt.plot(index, ed2, label =\"ED \")\n",
    "    plt.plot(index,ed3, label = \"ET\")\n",
    "    plt.title(\"Validation Dice Scores per class: (Sample_size={}, lr={})\".format(BATCH_SIZE*SLICE_NUM ,LEARNING_RATE))\n",
    "    plt.xlabel(\"Epoch:\")\n",
    "    plt.ylabel(\"Mean Dice Scores: \")\n",
    "    plt.legend()\n",
    "    plt.savefig(MODEL_PATH+ 'Eval dice per class plot.png')\n",
    "    plt.show()\n",
    "    test(test_data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52299885",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edc9ca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded!\n",
      "Epoch: 0\n",
      "Training in session: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/109 [00:02<05:21,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_id:  0 Training loss:  0.8565486669540405\n",
      "Batch_id:  0 Training IoU:  tensor(0.0646, device='cuda:0')\n",
      "Batch_id:  0 Training Mean Dice Score:  tensor(0.4747, device='cuda:0')\n",
      "Batch_id:  0 Training Non-tumor Dice Score:  tensor(0.2608, device='cuda:0')\n",
      "Batch_id:  0 Training NCR/NET Dice Score:  tensor(0.9601, device='cuda:0')\n",
      "Batch_id:  0 Training ED Dice Score:  tensor(0.4570, device='cuda:0')\n",
      "Batch_id:  0 Training ET Dice Score:  tensor(0.7463, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 2/109 [00:04<03:35,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_id:  1 Training loss:  0.8559902310371399\n",
      "Batch_id:  1 Training IoU:  tensor(0.1223, device='cuda:0')\n",
      "Batch_id:  1 Training Mean Dice Score:  tensor(0.5626, device='cuda:0')\n",
      "Batch_id:  1 Training Non-tumor Dice Score:  tensor(0.3700, device='cuda:0')\n",
      "Batch_id:  1 Training NCR/NET Dice Score:  tensor(0.9864, device='cuda:0')\n",
      "Batch_id:  1 Training ED Dice Score:  tensor(0.6981, device='cuda:0')\n",
      "Batch_id:  1 Training ET Dice Score:  tensor(0.6332, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8704\\4145655691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \"\"\"\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.71\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8704\\3757788889.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(BEST_EVAL)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Epoch: {i}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdi0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdi1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdi2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdi3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0meacc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meloss_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0memd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0medi0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0medi1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0medi2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0medi3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8704\\693305416.py\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(dataset, model, opt, dev, batch_size, lr)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mepoch_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# reshape the data and load the data to gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8704\\2921603484.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINPUT_CHANNEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_cat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8704\\2921603484.py\u001b[0m in \u001b[0;36mget_image\u001b[1;34m(image_path, image_cat, image_id, image_type)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Simple helper function for retriving images given the path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_cat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mt1_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_cat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_t1.nii.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mt1ce_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_cat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_t1ce.nii.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mt2_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_cat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_t2.nii.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\nibabel\\dataobj_images.py\u001b[0m in \u001b[0;36mget_fdata\u001b[1;34m(self, caching, dtype)\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;31m# For array proxies, will attempt to confine data array to dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;31m# during scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcaching\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'fill'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fdata_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\nibabel\\arrayproxy.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    368\u001b[0m             \u001b[0mScaled\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \"\"\"\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_scaled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslicer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\nibabel\\arrayproxy.py\u001b[0m in \u001b[0;36m_get_scaled\u001b[1;34m(self, dtype, slicer)\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[0mscl_inter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscl_inter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;31m# Read array and upcast as necessary for big slopes, intercepts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mscaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_read_scaling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_unscaled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslicer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslicer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscl_slope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscl_inter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0mscaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpromote_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\nibabel\\arrayproxy.py\u001b[0m in \u001b[0;36m_get_unscaled\u001b[1;34m(self, slicer)\u001b[0m\n\u001b[0;32m    314\u001b[0m                                        \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m                                        \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m                                        mmap=self._mmap)\n\u001b[0m\u001b[0;32m    317\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_fileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m             return fileslice(fileobj,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\site-packages\\nibabel\\volumeutils.py\u001b[0m in \u001b[0;36marray_from_file\u001b[1;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'readinto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mdata_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[0mn_read\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_bytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m         \u001b[0mneeds_copy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\FYP\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m             \u001b[0muncompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "yellow - label 4\n",
    "dark blue - label 1 \n",
    "green - label 2 \n",
    "\n",
    "purple - label 0\n",
    "\"\"\"\n",
    "\n",
    "main(0.71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da77e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d2144b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16720\\2299830927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meval_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGetBraTS19Dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCSV_FILE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIMAGE_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m't1ce'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m't2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "Perform feature selection on masks generated from train/eval/test dataset and \n",
    "the original ground truth masks\n",
    "\n",
    "Then use SVM ensemble methods to form a regression model to generate predication\n",
    "and compare with the those generated from the ground truth masks and \n",
    "the ground truth survial days\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030d85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
